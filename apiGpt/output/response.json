{
    "exam": {
        "multiple_choice": [
            {
                "question": "What is the purpose of Cross-Validation?",
                "options": [
                    "To assess how the results of a statistical analysis will generalize to an independent data set",
                    "To regularize the model and prevent overfitting",
                    "To reduce the dimensionality of the dataset",
                    "To optimize hyperparameters without grounded data"
                ],
                "answer": "To assess how the results of a statistical analysis will generalize to an independent data set"
            },
            {
                "question": "Which method is often used in supervised learning?",
                "options": [
                    "K-Means Clustering",
                    "Naive Bayes Classifier",
                    "Principal Component Analysis",
                    "Hierarchical Clustering"
                ],
                "answer": "Naive Bayes Classifier"
            },
            {
                "question": "What is an advantage of decision trees?",
                "options": [
                    "They require extensive data preprocessing",
                    "They can model only linear relationships",
                    "They are easy to interpret and visualize",
                    "They perform well for high-dimensional data"
                ],
                "answer": "They are easy to interpret and visualize"
            },
            {
                "question": "Which of the following is a disadvantage of using K-Nearest Neighbors?",
                "options": [
                    "It is sensitive to the curse of dimensionality",
                    "It requires labeled data for training",
                    "It is robust to outliers",
                    "It can handle both categorical and numerical data"
                ],
                "answer": "It is sensitive to the curse of dimensionality"
            },
            {
                "question": "What does the term 'overfitting' refer to?",
                "options": [
                    "A model that performs well on unseen data",
                    "A model that is too simple to capture the underlying data patterns",
                    "A model that fits the training data too closely and fails on new data",
                    "A model that ignores the training data completely"
                ],
                "answer": "A model that fits the training data too closely and fails on new data"
            },
            {
                "question": "In what scenario would you use a support vector machine?",
                "options": [
                    "When the classes are linearly separable",
                    "When you want to reduce the size of a dataset",
                    "When there are categorical features only",
                    "When the dataset is small and unlabeled"
                ],
                "answer": "When the classes are linearly separable"
            },
            {
                "question": "What is a common technique for feature selection?",
                "options": [
                    "Cross-Validation",
                    "Grid Search",
                    "Principal Component Analysis",
                    "Neural Network Training"
                ],
                "answer": "Principal Component Analysis"
            },
            {
                "question": "Which algorithm is primarily used for unsupervised learning?",
                "options": [
                    "Logistic Regression",
                    "K-Means Clustering",
                    "Support Vector Machine",
                    "Random Forest"
                ],
                "answer": "K-Means Clustering"
            }
        ],
        "open_questions": [
            {
                "question": "Explain the importance of hyperparameter tuning in machine learning.",
                "answer": "Hyperparameter tuning is crucial as it helps in finding the best performance for a model by optimizing the parameters that govern the training process. These parameters can significantly affect the model's performance and its ability to generalize to unseen data."
            },
            {
                "question": "Discuss the difference between bagging and boosting in ensemble learning.",
                "answer": "Bagging reduces variance by training multiple models independently and averaging their predictions, while boosting reduces bias by combining multiple weak learners sequentially, where each model focuses on correcting errors made by the previous ones."
            },
            {
                "question": "Describe a scenario where using a neural network is preferable over traditional models.",
                "answer": "Neural networks are preferable in scenarios involving complex patterns and large datasets, such as image recognition or natural language processing, where traditional models might struggle to capture the intricacies of the data."
            }
        ]
    }
}